# 增强版Commit审查功能使用说明

## 概述

增强版Commit审查功能是对原有simple模式的重大升级，旨在解决在实际使用过程中可能遇到的各种失误和问题。该功能提供了多层次的防错机制，确保审查过程的稳定性和可靠性。

## 主要特性

### 1. 智能Token长度预检查
- 在发送请求给LLM之前，自动估算输入内容的token数量
- 防止超出LLM的上下文限制导致的截断或失败
- 根据预估结果自动决定是否采用分批处理策略

### 2. 占位符替换失败的自动回退机制
- 检测LLM响应中是否包含所有必需的占位符
- 当占位符缺失时，自动在相应位置补充内容
- 提供详细的失败统计和提示信息

### 3. 分批处理策略
- 当文件数量过多或内容过长时，自动采用分批处理
- 每批处理固定数量的文件，确保每次请求都在合理范围内
- 最终将所有批次的结果汇总成完整的审查报告

### 4. 响应格式验证
- 验证LLM响应是否包含必要的章节结构
- 检查响应的完整性和格式正确性
- 提供详细的验证报告和错误提示

### 5. 渐进式降级处理
- 当一次性处理失败时，自动降级到分批处理
- 当增强版处理失败时，自动回退到基础版本
- 确保在任何情况下都能提供可用的审查结果

## 配置选项

### 基本配置

```python
# 启用增强版commit审查功能
ENABLE_ENHANCED_COMMIT_REVIEW = True

# 审查模式设置（当为'simple'时生效）
COMMIT_REVIEW_MODE = "simple"
```

### 高级配置

```python
# 触发分批处理的预估token阈值
MAX_ESTIMATED_TOKENS = 50000

# 分批处理时每批的文件数量
BATCH_SIZE_FOR_COMMIT_REVIEW = 5

# 占位符缺失超过此比例时触发降级处理
INCOMPLETE_RESPONSE_THRESHOLD = 0.5
```

## 配置参数详解

### MAX_ESTIMATED_TOKENS
- **默认值**: 50000
- **作用**: 当预估的输入token数量超过此值时，自动采用分批处理策略
- **建议**: 根据您使用的LLM模型的上下文限制进行调整
  - GPT-3.5: 建议设置为 30000-40000
  - GPT-4: 建议设置为 50000-80000  
  - Gemini: 建议设置为 100000-150000
  - Kimi: 建议设置为 120000-180000

### BATCH_SIZE_FOR_COMMIT_REVIEW
- **默认值**: 5
- **作用**: 分批处理时每批包含的文件数量
- **建议**: 根据文件复杂度和LLM性能调整
  - 简单文件: 可设置为 6-8
  - 复杂文件: 建议设置为 3-5
  - 大型文件: 建议设置为 2-3

### INCOMPLETE_RESPONSE_THRESHOLD
- **默认值**: 0.5 (50%)
- **作用**: 当占位符缺失比例超过此值时，触发降级处理
- **建议**: 根据对完整性的要求调整
  - 严格模式: 设置为 0.2 (20%)
  - 标准模式: 设置为 0.5 (50%)
  - 宽松模式: 设置为 0.8 (80%)

## 使用示例

### 场景1: 小型Commit（推荐配置）
```python
ENABLE_ENHANCED_COMMIT_REVIEW = True
MAX_ESTIMATED_TOKENS = 30000
BATCH_SIZE_FOR_COMMIT_REVIEW = 6
INCOMPLETE_RESPONSE_THRESHOLD = 0.3
```

### 场景2: 大型Commit（稳定性优先）
```python
ENABLE_ENHANCED_COMMIT_REVIEW = True
MAX_ESTIMATED_TOKENS = 20000
BATCH_SIZE_FOR_COMMIT_REVIEW = 3
INCOMPLETE_RESPONSE_THRESHOLD = 0.2
```

### 场景3: 高性能LLM（效率优先）
```python
ENABLE_ENHANCED_COMMIT_REVIEW = True
MAX_ESTIMATED_TOKENS = 80000
BATCH_SIZE_FOR_COMMIT_REVIEW = 8
INCOMPLETE_RESPONSE_THRESHOLD = 0.5
```

## 工作流程

1. **预检查阶段**
   - 过滤需要审查的文件
   - 估算token数量
   - 决定处理策略

2. **处理阶段**
   - 根据策略选择一次性处理或分批处理
   - 发送请求给LLM
   - 获取响应内容

3. **验证阶段**
   - 验证响应格式和完整性
   - 检查占位符是否齐全
   - 评估响应质量

4. **后处理阶段**
   - 替换占位符为实际内容
   - 构建最终审查报告
   - 提供统计信息

5. **降级处理**
   - 检测处理失败情况
   - 自动选择降级策略
   - 确保始终有可用结果

## 日志解读

### 正常处理日志
```
📝 开始增强版审查commit: abc12345 - 添加新功能 (by developer)
📊 预估输入token数: 25000
📝 开始LLM分析commit abc12345，包含 8 个文件
📊 LLM响应: 3500 tokens, 12500 字符
✅ AI响应完整，包含所有 8 个文件的占位符
📊 占位符替换统计: 8/8 成功
✅ 增强版Commit abc12345 审查完成
```

### 降级处理日志
```
📝 开始增强版审查commit: def67890 - 重构代码 (by developer)
📊 预估输入token数: 65000
⚠️ 预估token过多(65000)，超过阈值(50000)，将采用分批处理策略
📝 开始分批审查commit: def67890
📝 处理第 1/3 批，包含 5 个文件
✅ 完成第1批审查，tokens: 2800
📝 处理第 2/3 批，包含 5 个文件
✅ 完成第2批审查，tokens: 2600
📝 处理第 3/3 批，包含 4 个文件
✅ 完成第3批审查，tokens: 2400
✅ 分批处理Commit def67890 审查完成，总tokens: 7800
```

### 错误处理日志
```
📝 开始增强版审查commit: ghi90123 - 修复bug (by developer)
❌ LLM返回内容为空 (commit review) for ghi90123
⚠️ **审查提示**: LLM返回内容为空，请稍后重试
```

## 性能优化建议

1. **根据LLM模型调整参数**
   - 不同模型有不同的上下文限制和性能特性
   - 建议先用小的测试数据集进行参数调优

2. **监控token使用情况**
   - 定期检查日志中的token使用统计
   - 根据实际使用情况调整阈值

3. **考虑成本因素**
   - 分批处理会增加API调用次数
   - 在成本和可靠性之间找到平衡点

4. **测试不同场景**
   - 小型commit（1-3个文件）
   - 中型commit（4-10个文件）
   - 大型commit（10+个文件）

## 故障排除

### 常见问题

1. **Q: 为什么总是触发分批处理？**
   - A: 检查 `MAX_ESTIMATED_TOKENS` 设置是否过低
   - A: 确认文件大小和diff复杂度是否合理

2. **Q: 占位符替换失败率过高怎么办？**
   - A: 降低 `INCOMPLETE_RESPONSE_THRESHOLD` 值
   - A: 减少 `BATCH_SIZE_FOR_COMMIT_REVIEW` 数量

3. **Q: 审查结果不完整怎么办？**
   - A: 启用增强版功能会自动处理这种情况
   - A: 检查LLM服务是否稳定

### 调试技巧

1. 启用详细日志
2. 监控token使用统计
3. 检查占位符替换成功率
4. 观察降级处理触发频率

## 总结

增强版Commit审查功能通过多层次的防错机制，显著提高了代码审查的稳定性和可靠性。通过合理的配置和使用，可以在各种场景下提供一致的审查体验。

建议在生产环境中启用此功能，并根据实际使用情况调整配置参数，以获得最佳的性能和可靠性平衡。 